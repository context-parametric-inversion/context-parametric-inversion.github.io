<!DOCTYPE html>
<html>

<head>
  <!-- Google tag (gtag.js) -->
  <link rel="icon" href="./resources/mars.png" type="image/png">

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-BM130PWZB5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-BM130PWZB5');
  </script>
  <meta charset="utf-8">
  <meta name="description" content="Context Parametric Inversion">
  <meta name="keywords" content="Instruction Finetuning, knowledge Conflicts">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Content-Parametric Inversion with Instruction Finetuning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
    integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <link rel="icon" href="./favicon.ico?">

</head>

<body>

  <section class="hero" style="background-color: #B22234;">
    <div class="hero-body no-bottom-padding">
      <div class="container">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title" style="color: white;">
              Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance
            </h1>
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block" style="color: white;">
                <a target="_blank" href="https://saching007.github.io/" style="color:blanchedalmond !important;">Sachin
                  Goyal</a><sup>*</sup>&nbsp;&nbsp;&nbsp;
                <a target="_blank" href="https://kebaek.github.io/" style="color:blanchedalmond !important;">Christina
                  Baek</a><sup>*</sup>&nbsp;&nbsp;&nbsp;
                <a target="_blank" href="https://zicokolter.com/" style="color:blanchedalmond !important;">Zico
                  Kolter</a>&nbsp;&nbsp;&nbsp;
                <a target="_blank" href="https://www.cs.cmu.edu/~aditirag/"
                  style="color:blanchedalmond !important;">Aditi Raghunathan</a>
                <br /><sup>*</sup>Equal Contribution&nbsp;&nbsp;&nbsp;Carnegie Mellon University
              </span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- arXiV Link. -->
                <span class="link-block">
                  <a target="_blank" href="https://arxiv.org/abs/2410.10796"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- GitHub Link. -->
                <span class="link-block">
                  <a target="_blank" href="https://github.com/locuslab/context-parametric-inversion"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>GitHub</span>
                  </a>
                </span>
                <span class="link-block">
                  <a target="_blank" href="https://twitter.com/goyalsachin007/status/1677314439236681730?s=20"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-twitter"></i>
                    </span>
                    <span>Summary</span>
                  </a>
                </span>
              </div>

            </div>
          </div>

        </div>
      </div>
    </div>
    </div>
    </div>
  </section>


  <section class="section">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-two-thirds">
          <!-- <h2 class="title is-2" style="text-align: center; padding-bottom: 10px;">TLDR</h2> -->

          <tr>
            <td>

              <h2 class="title is-2" style="text-align: center; padding-bottom: 10px;">TLDR</h2>
              <p>
                We uncover a broad failure in Instruction Finetuning of Large Language Models, where the model begins to
                rely more on it's parametric knowledge than the input context when they conflict, despite an initial
                jump in context reliance. We call this as <b>context-parametric inversion</b>.
              </p>
              <hr color="gray" size="3">


              <h2 class="title is-2" style="text-align: center; padding-bottom: 10px;">Context-Parametric Inversion</h2>
              <div class="row">
                <div class="col" style="text-align: center">
                  <!-- add center align the image -->
                  <img src="./resources/cpi.png" width="50%" style="border-radius:10px;"></img>
                </div>
              </div>
              <p style="margin-top: 20px;">
                LLM's are instruction finetuned to improve their ability to follow the user instructions and the input
                context.
                But why do even the instruction tuned chatbots still fail to follow the input context, especially when
                it
                conflicts their pretraining knowledge? In principle, instruction finetuning (IFT) should improve model's
                ability to follow the input context over its parametric knowledge, but we observe an intriguing and
                counterintuitive behavior instead.
              <ol>
                <li>Across the IFT trajectory, we measure model's context reliance i.e. it's ability to answer based on
                  the input context over it's parametric knowledge, when the two are at conflicts.</li>
                <li>Contrary to the expectation, context reliance infact
                  decreases with instruction finetuning, despite an initial expected increase.
                  This decrease happens, while the performance on standard benchmarks keeps on increasing (figure
                  above).</li>
                <li>We call this as <b>context-parametric inversion</b> and observe this across multiple general-purpose
                  IFT datasets.
                </li>

              </ol>
              </p>
              <hr color="gray" size="3">




              <h2 class="title is-2" style="text-align: center; padding-bottom: 10px;">Is there a Simple Explanation
                for this?</h2>
              <!-- center align the h3 -->
              <h3 class="subtitle is-3" style="text-align: center; padding-bottom: 10px;">Negating some simple
                hypotheses
              </h3>
              <div class="row">
                <div class="col" style="text-align: center">
                  <img src="./resources/alpaca_filter.png"
                    style="border: 0px solid #bbb; border-radius: 10px; width: 100%;"></img></a>
                </div>
              </div>
              <p style="margin-top: 20px;">
                Through various controlled studies, we show that context-parametric inversion during IFT cannot be
                simply explained by classical hypotheses like memorization, overfitting or forgetting.
              <ol>
                <li>The increasing reliance on parametric knowledge (drop in context reliance) extends to facts beyond
                  those seen during finetuning. Removing any overlap between the finetuning and evaluation set does not
                  mitigate the drop in context reliance (figure on the left).</li>
                <li>Another hypotheses could be lack of enough context based answering datapoints in
                  instruction finetuning dataset. However, even when we finetune on a context-only subset of
                  IFT data (e.g., Alpaca), we observe a drop in context reliance (right figure, red-curve).</li>
                <li> This highlights that not all context-based datapoints effectively promote context reliance, and
                  model still seems to learn alternative preditive features.</li>
              </ol>
              </p>

              <hr color="gray" size="3">

              <br>
              <h2 class="title is-2" style="text-align: center; padding-bottom: 10px;">What causes context-parametric
                inversion?</h2>

              <p>We saw above that finetuning only on a context-based subset of IFT data still leads to a drop in
                context.
                This highlights that not all context-based datapoints promote context reliance. We take a closer look at
                the composition of instruction finetuning datasets below.</p>

              <!-- <h3 class="subtitle is-3" style="text-align: center; padding-bottom: 10px;">A closer look at the
                Instruction Finetuning Datasets
              </h3> -->
              <div class="row">
                <div class="col" style="text-align: center">
                  <!-- add center align the image -->
                  <img src="./resources/data_cat.png" width="70%" style="border-radius:10px;"></img>
                </div>
              </div>
              <p style="margin-top: 20px;">
              <ol>
                <li><b>Context-Critical Datapoints</b>: Context provides key information needed to answer the user
                  query.</li>
                <li><b>Non-Context Critical Datapoints</b>: Context is approximately redundant with modelâ€™s parametric
                  knowledge.
                </li>
              </ol>
              We refer the reader to Section 4.3 in the paper for further details around how will split IFT data into
              these two categories.
              <!-- <h3 class="subtitle is-3" style="text-align: center; padding-bottom: 10px;">What causes context-parametric
                inversion? </h3> -->
              Both <b>empirically and theoretically</b>, we show that this drop in context reliance is due to the
              presence of non-context critical datapoints. <i>In the early stages of training</i>, context-critical
              points
              have a high loss, and drive the attention towards the context. However, as training progresses, the
              loss on context-critical points decreases, and the non-context-critical points dominate the gradient.
              <i>In the later stages</i>, the model
              leverages its pretrained knowledge to further reduce the loss on the <i>non-context-critical points</i>,
              shifting the attention away from the context.
              </p>


              <hr color="gray" size="3">

              <br>
              <h2 class="title is-2" style="text-align: center; padding-bottom: 10px;">Does Counterfactual Data
                Augmentation Help?</h2>
              <div class="row">
                <div class="col" style="text-align: center">
                  <!-- add center align the image -->
                  <img src="./resources/mitigation.png" width="70%" style="border-radius:10px;"></img>
                </div>
              </div>
              <p style="margin-top: 20px;">
                Our theoretical analysis naturally leads us to some potential mitigation strategies beyond filtering out
                non-context critical datapoints. These strategies give some limited but insightful gains.
              <ol>
                <li>Counterfactual data augmentation, a widely used approach, improves context reliance <i>only</i> on
                  tasks similar in type to the augmented data. </li>
                <li> For example, adding entity substituted QA counterfactual data improves performance on QA tasks
                  only (e.g., CF_BIO) and doesn't generalize to other kind of context-parametric conflicts (e.g.,
                  CF_Quotes).</li>
                <li><b>QK Finetuning: </b> Regularizing by updating only the "Query" and "Key"
                  matrices enhances context reliance on certain tasks but may hurt performance on standard benchmarks,
                  as value matrices can learn additional facts during finetuning.</li>
              </ol>
              </p>

              <p> Overall, we hope that our analysis serves as a starting point in addressing this fundamental challenge
                of improving context reliance in LLMs and the effect of instruction finetuning on the
                same.</p>


        </div>
        </td>
        </tr>
      </div>
    </div>
    </div>
  </section>



  <br>
  <br>
  <!-- Insert a footer now in CMU red colour -->
  <footer class="footer" style="background-color: #B22234;">
    <div class="content has-text-centered">
      <p style="color: white;">
        <bold>CMU</bold> &copy; 2024. All rights reserved.
      </p>
    </div>
  </footer>

</body>

</html>